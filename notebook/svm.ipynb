{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('usr')"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# SVM Notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### modules\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from typing import Union, Optional"
   ]
  },
  {
   "source": [
    "### Trainingsdaten"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dict(np.load(\"two_moons.npz\", allow_pickle=True))\n",
    "train_samples = train_data[\"samples\"]\n",
    "train_labels = train_data[\"labels\"]\n",
    "# we need to change the labels for class 0 to -1 to account for the different labels used by an SVM \n",
    "train_labels[train_labels == 0] = -1 \n",
    "\n",
    "test_data = dict(np.load(\"two_moons_test.npz\", allow_pickle=True))\n",
    "test_samples = test_data[\"samples\"]\n",
    "test_labels = test_data[\"labels\"]\n",
    "# we need to change the labels for class 0 to -1 to account for the different labels used by an SVM \n",
    "test_labels[test_labels == 0] = -1\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Train Data\")\n",
    "plt.scatter(x=train_samples[train_labels == -1, 0], y=train_samples[train_labels == -1, 1], label=\"c=-1\", c=\"blue\")\n",
    "plt.scatter(x=train_samples[train_labels == 1, 0], y=train_samples[train_labels == 1, 1], label=\"c=1\", c=\"orange\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Test Data\")\n",
    "plt.scatter(x=test_samples[test_labels == -1, 0], y=test_samples[test_labels == -1, 1], label=\"c=-1\", c=\"blue\")\n",
    "plt.scatter(x=test_samples[test_labels == 1, 0], y=test_samples[test_labels == 1, 1], label=\"c=1\", c=\"orange\")\n",
    "plt.legend()"
   ]
  },
  {
   "source": [
    "## 1. SVM mit Kernel\n",
    "\n",
    "CVXPY wird zur Lösung der Wolfe Dual genutzt."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "def solve_qp(Q: np.ndarray, q: np.ndarray,\n",
    "             G:np.ndarray, h: Union[np.ndarray, float],\n",
    "             A:np.ndarray, b: Union[np.ndarray, float]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    solves quadratic problem: min_x  0.5x^T Q x + q.^T x s.t. Gx <= h and Ax = b\n",
    "      in the following 'dim' refers to the dimensionality of the optimization variable x\n",
    "    :param Q: matrix of the quadratic term of the objective, (shape [dim, dim])\n",
    "    :param q: vector for the linear term of the objective, (shape [dim])\n",
    "    :param G: factor for lhs of the inequality constraint (shape [dim], or [dim, dim])\n",
    "    :param h: rhs of the inequality constraint (shape [dim], or scalar)\n",
    "    :param A: factor for lhs of the equality constraint (shape [dim], or [dim, dim])\n",
    "    :param b: rhs of the equality constraint (shape [dim], or scalar)\n",
    "    :return: optimal x (shape [dim])\n",
    "    \"\"\"\n",
    "    x = cp.Variable(q.shape[0])\n",
    "    prob = cp.Problem(cp.Minimize(0.5 * cp.quad_form(x, Q) + q.T @ x), constraints=[G @ x <= h, A @ x == b])\n",
    "    prob.solve()\n",
    "    return x.value"
   ]
  },
  {
   "source": [
    "Kernel definieren und Funktionen zur Konstruktion und Ausführung der SVM zur Verfügung stellen."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_gaussian_kernel_matrix(x: np.ndarray, sigma: float, y: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "    \"\"\" Computes Kernel matrix K(x,y) between two sets of data points x, y  for a Gaussian Kernel with bandwidth sigma.\n",
    "    If y is not given it is assumed to be equal to x, i.e. K(x,x) is computed\n",
    "    :param x: matrix containing first set of points (shape: [N, data_dim])\n",
    "    :param sigma: bandwidth of gaussian kernel\n",
    "    :param y: matrix containing second set of points (shape: [M, data_dim])\n",
    "    :return: kernel matrix K(x,y) (shape [M, N])\n",
    "    \"\"\"\n",
    "    if y is None:\n",
    "        y = x\n",
    "    result = np.zeros((y.shape[0],x.shape[0]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[0]):\n",
    "            result[j,i] = np.dot(x[i]-y[j], x[i]-y[j])\n",
    "    return np.exp(-sigma**(-2) * result)\n",
    "\n",
    "\n",
    "\n",
    "def fit_svm(samples: np.ndarray, labels: np.ndarray, sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    fits an svm (with Gaussian Kernel)\n",
    "    :param samples: samples to fit the SVM to (shape: [N, data_dim])\n",
    "    :param labels: class labels corresponding to samples (shape: [N])\n",
    "    :param sigma: bandwidth of gaussian kernel\n",
    "    :return: \"alpha\" values, weight for each datapoint in the dual formulation of SVM (shape [N])\n",
    "    \"\"\"\n",
    "    N = labels.shape[0]\n",
    "    Q = get_gaussian_kernel_matrix(samples, sigma) * (labels @ labels.T)\n",
    "    q = -1 * np.ones((N)).T\n",
    "    A = labels.T\n",
    "    b = 0\n",
    "    G = (-1) * np.identity(N)\n",
    "    h = np.zeros((N)).T\n",
    "    return solve_qp(Q,q,G,h,A,b)\n",
    "\n",
    "\n",
    "\n",
    "def predict_svm(samples_query: np.ndarray, samples_train: np.ndarray, labels_train: np.ndarray,\n",
    "                alphas: np.ndarray, sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    predict labels for query samples given training data and weights\n",
    "    :param samples_query: samples to query (i.e., predict labels) (shape: [N_query, data_dim])\n",
    "    :param samples_train: samples that where used to train svm (shape: [N_train, data_dim])\n",
    "    :param labels_train: labels corresponding to samples that where used to train svm (shape: [N_train])\n",
    "    :param alphas: alphas computed by training procedure (shape: [N_train])\n",
    "    :param sigma: bandwidth of gaussian kernel\n",
    "    :return: predicted labels for query points (shape: [N_query])\n",
    "    \"\"\"\n",
    "    return np.sign((alphas * labels_train) @ get_gaussian_kernel_matrix(samples_query,sigma,y=samples_train))"
   ]
  },
  {
   "source": [
    "Ausführung und Visualisierung"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.3\n",
    "\n",
    "# train\n",
    "alphas = fit_svm(train_samples, train_labels, sigma)\n",
    "\n",
    "# evaluate \n",
    "train_predictions = predict_svm(train_samples, train_samples, train_labels, alphas, sigma)\n",
    "test_predictions = predict_svm(test_samples, train_samples, train_labels, alphas, sigma)\n",
    "\n",
    "predicted_train_labels = np.ones(train_predictions.shape)\n",
    "predicted_train_labels[train_predictions < 0] = -1\n",
    "print(\"Train Accuracy: \", np.count_nonzero(predicted_train_labels == train_labels) / len(train_labels))\n",
    "\n",
    "predicted_test_labels = np.ones(test_predictions.shape)\n",
    "predicted_test_labels[test_predictions < 0] = -1\n",
    "print(\"Test Accuracy: \", np.count_nonzero(predicted_test_labels == test_labels) / len(test_labels))\n",
    "\n",
    "# plot train, contour, decision boundary and margins \n",
    "plt.figure()\n",
    "plt_range = np.arange(-1.5, 2.5, 0.01)\n",
    "plt_grid = np.stack(np.meshgrid(plt_range, plt_range), axis=-1)\n",
    "flat_plt_grid = np.reshape(plt_grid, [-1, 2])\n",
    "plt_grid_shape = plt_grid.shape[:2]\n",
    "\n",
    "pred_grid = np.reshape(predict_svm(flat_plt_grid, train_samples, train_labels, alphas, sigma), plt_grid_shape)\n",
    "plt.contour(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=[-1, 0, 1], colors=('blue', 'black', 'orange'),\n",
    "             linestyles=('-',), linewidths=(2,))\n",
    "plt.contourf(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=10)\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.scatter(x=train_samples[train_labels == -1, 0], y=train_samples[train_labels == -1, 1], label=\"c=-1\", c=\"blue\")\n",
    "plt.scatter(x=train_samples[train_labels == 1, 0], y=train_samples[train_labels == 1, 1], label=\"c=1\", c=\"orange\")\n",
    "plt.legend()\n",
    "\n",
    "# plot margin, decision boundary and support vectors\n",
    "plt.figure()\n",
    "plt.contour(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=[-1, 0, 1], colors=('blue', 'black', 'orange'),\n",
    "             linestyles=('-',), linewidths=(2,))\n",
    "\n",
    "# squeeze alpha values into interval [0, 1] for plotting\n",
    "alphas_plt = np.clip(alphas / np.max(alphas), a_min=0.0, a_max=1.0)\n",
    "for label, color in zip([-1, 1], [\"blue\", \"orange\"]):\n",
    "    color_rgb = colors.to_rgb(color)\n",
    "    samples = train_samples[train_labels == label]\n",
    "    color_rgba = np.zeros((len(samples), 4))\n",
    "    color_rgba[:, :3] = color_rgb\n",
    "    color_rgba[:, 3] = alphas_plt[train_labels == label]\n",
    "    plt.scatter(x=samples[:, 0], y=samples[:, 1], c=color_rgba)\n",
    "\n",
    "\n",
    "plt.xlim(-1.5, 2.5)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## 2. SVM mit Hinge-loss"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Transformation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubic_feature_fn(samples: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param x: Batch of 2D data vectors [x, y] [N x dim]\n",
    "    :return cubic features: [x**3, y**3, x**2 * y, x * y**2, x**2, y**2, x*y, x, y, 1]\n",
    "    \"\"\"\n",
    "    x = samples[..., 0]\n",
    "    y = samples[..., 1]\n",
    "    return np.stack([x**3, y**3, x**2 * y, x * y**2, x**2, y**2, x*y, x, y, np.ones(x.shape[0])], axis=-1)"
   ]
  },
  {
   "source": [
    "### Hinge-loss und Gradient"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm(weights: np.ndarray, features: np.ndarray, labels: np.ndarray, slack_regularizer: float) -> float:\n",
    "    \"\"\"\n",
    "    objective for svm training with hinge loss\n",
    "    :param weights: current weights to evaluate (shape: [feature_dim])\n",
    "    :param features: features of training samples (shape:[N x feature_dim])\n",
    "    :param labels: class labels corresponding to train samples (shape: [N])\n",
    "    :param slack_regularizer: Factor to weight the violation of margin with (C in slides)\n",
    "    :returns svm (hinge) objective (scalar)\n",
    "    \"\"\"\n",
    "    pred = 1 - labels * (features @ weights)\n",
    "    return np.dot(weights,weights) + slack_regularizer * pred[pred > 0].sum()\n",
    "\n",
    "\n",
    "def d_objective_svm(weights: np.ndarray, features: np.ndarray, labels: np.ndarray, slack_regularizer: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    gradient of objective for svm training with hinge loss\n",
    "    :param weights: current weights to evaluate (shape: [feature_dim])\n",
    "    :param features: features of training samples (shape: [N x feature_dim])\n",
    "    :param labels: class labels corresponding to train samples (shape: [N])\n",
    "    :param slack_regularizer: Factor to weight the violation of margin with (C in slides)\n",
    "    :returns gradient of svm objective (shape: [feature_dim])\n",
    "    \"\"\"\n",
    "    decider = labels * (features @ weights)\n",
    "    \n",
    "    dLoss = -1 * labels[:,None] * features\n",
    "    dLoss[decider >= 1] = np.zeros(weights.shape)\n",
    "\n",
    "    return 2*weights + slack_regularizer * dLoss.sum(axis=0)"
   ]
  },
  {
   "source": [
    "### Train and evaluate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "feature_fn = cubic_feature_fn\n",
    "C = 30\n",
    "\n",
    "# optimization\n",
    "\n",
    "train_features = feature_fn(train_samples)\n",
    "\n",
    "# For detail see: https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html\n",
    "res = opt.minimize(\n",
    "    # pass objective\n",
    "    fun=lambda w: objective_svm(w, train_features, train_labels, C),\n",
    "    # pass initial point              \n",
    "    x0=np.ones(train_features.shape[-1]),\n",
    "    # pass function to evaluate gradient (in scipy.opt \"jac\" for jacobian)\n",
    "    jac=lambda w: d_objective_svm(w, train_features, train_labels, C),\n",
    "    # specify method to use\n",
    "    method=\"l-bfgs-b\")\n",
    "\n",
    "print(res)\n",
    "w_svm = res.x\n",
    "\n",
    "# evaluation \n",
    "test_predictions = feature_fn(test_samples) @ w_svm\n",
    "train_predictions = feature_fn(train_samples) @ w_svm\n",
    "\n",
    "predicted_train_labels = np.ones(train_predictions.shape)\n",
    "predicted_train_labels[train_predictions < 0] = -1\n",
    "print(\"Train Accuracy: \", np.count_nonzero(predicted_train_labels == train_labels) / len(train_labels))\n",
    "\n",
    "predicted_test_labels = np.ones(test_predictions.shape)\n",
    "predicted_test_labels[test_predictions < 0] = -1\n",
    "print(\"Test Accuracy: \", np.count_nonzero(predicted_test_labels == test_labels) / len(test_labels))\n",
    "\n",
    "# plot train, contour, decision boundary and margins \n",
    "plt.figure()\n",
    "plt.title(\"Max Margin Solution\")\n",
    "plt_range = np.arange(-1.5, 1.5, 0.01)\n",
    "plt_grid = np.stack(np.meshgrid(plt_range, plt_range), axis=-1)\n",
    "flat_plt_grid = np.reshape(plt_grid, [-1, 2])\n",
    "plt_grid_shape = plt_grid.shape[:2]\n",
    "\n",
    "pred_grid = np.reshape(feature_fn(flat_plt_grid) @ w_svm, plt_grid_shape)\n",
    "\n",
    "#plt.contour(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=[-1.0, 0.0, 1.0], colors=[\"blue\", \"black\", \"orange\"])\n",
    "plt.contour(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=[-1, 0, 1], colors=('blue', 'black', 'orange'),\n",
    "             linestyles=('-',), linewidths=(2,))\n",
    "plt.contourf(plt_grid[..., 0], plt_grid[..., 1], pred_grid, levels=10)\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "s0 =plt.scatter(x=train_samples[train_labels == -1, 0], y=train_samples[train_labels == -1, 1], label=\"c=-1\", c=\"blue\")\n",
    "s1 =plt.scatter(x=train_samples[train_labels == 1, 0], y=train_samples[train_labels == 1, 1], label=\"c=1\", c=\"orange\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.ylim(-1.5, 1.5)\n",
    "plt.show()"
   ]
  }
 ]
}